{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real time image segmentation\n",
    "\n",
    "The purpose of this notebook is to show how we can perform real time image segmentation using [Ultralytics](https://docs.ultralytics.com/) YOLO model.  \n",
    "Ultralytics provides a really simple API that allows to use powerful computer vision models optimized for real time use cases.  \n",
    "  \n",
    "We'll see how to:\n",
    "- Perform real time image segmentation.\n",
    "- Quantize the model in order to reduce its size and its inference time to make it more suitable for mobile or embedded devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLOv11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the YOLOv11 nano version which is the smaller version of the YOLOv11 model optimized for fast inference.  \n",
    "There are bigger versions that are more suitable if you want to maximize the model precision at the expense of inference time: see [YOLOv11 page](https://docs.ultralytics.com/models/yolo11/#overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model_name = 'yolo11n-seg'\n",
    "\n",
    "# Load YOLOv8 segmentation model\n",
    "model = YOLO(f\"{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 person, 266.8ms\n",
      "Speed: 6.6ms preprocess, 266.8ms inference, 16.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.1ms\n",
      "Speed: 3.0ms preprocess, 139.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 115.1ms\n",
      "Speed: 2.0ms preprocess, 115.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 109.7ms\n",
      "Speed: 2.0ms preprocess, 109.7ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 120.0ms\n",
      "Speed: 1.0ms preprocess, 120.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.0ms\n",
      "Speed: 2.0ms preprocess, 142.0ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 125.5ms\n",
      "Speed: 2.0ms preprocess, 125.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 128.3ms\n",
      "Speed: 2.0ms preprocess, 128.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.7ms\n",
      "Speed: 2.0ms preprocess, 132.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 130.1ms\n",
      "Speed: 1.5ms preprocess, 130.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 148.6ms\n",
      "Speed: 1.0ms preprocess, 148.6ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 168.1ms\n",
      "Speed: 2.0ms preprocess, 168.1ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 169.6ms\n",
      "Speed: 2.0ms preprocess, 169.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 148.1ms\n",
      "Speed: 2.0ms preprocess, 148.1ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 139.7ms\n",
      "Speed: 2.5ms preprocess, 139.7ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 125.2ms\n",
      "Speed: 3.1ms preprocess, 125.2ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 129.6ms\n",
      "Speed: 2.0ms preprocess, 129.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 147.1ms\n",
      "Speed: 2.0ms preprocess, 147.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 141.8ms\n",
      "Speed: 1.6ms preprocess, 141.8ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 162.9ms\n",
      "Speed: 2.5ms preprocess, 162.9ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 306.1ms\n",
      "Speed: 3.5ms preprocess, 306.1ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 189.4ms\n",
      "Speed: 3.0ms preprocess, 189.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 164.9ms\n",
      "Speed: 2.0ms preprocess, 164.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 150.2ms\n",
      "Speed: 2.0ms preprocess, 150.2ms inference, 4.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 145.6ms\n",
      "Speed: 2.0ms preprocess, 145.6ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 174.7ms\n",
      "Speed: 3.1ms preprocess, 174.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 138.1ms\n",
      "Speed: 3.0ms preprocess, 138.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 166.2ms\n",
      "Speed: 3.0ms preprocess, 166.2ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 121.9ms\n",
      "Speed: 1.6ms preprocess, 121.9ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 130.3ms\n",
      "Speed: 2.0ms preprocess, 130.3ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.0ms\n",
      "Speed: 2.0ms preprocess, 110.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.3ms\n",
      "Speed: 2.5ms preprocess, 142.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.6ms\n",
      "Speed: 2.0ms preprocess, 112.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 126.6ms\n",
      "Speed: 2.0ms preprocess, 126.6ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 207.7ms\n",
      "Speed: 2.0ms preprocess, 207.7ms inference, 4.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 176.0ms\n",
      "Speed: 2.6ms preprocess, 176.0ms inference, 5.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 135.0ms\n",
      "Speed: 3.0ms preprocess, 135.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 114.7ms\n",
      "Speed: 1.0ms preprocess, 114.7ms inference, 5.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.1ms\n",
      "Speed: 2.0ms preprocess, 146.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 159.7ms\n",
      "Speed: 2.0ms preprocess, 159.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 164.3ms\n",
      "Speed: 2.0ms preprocess, 164.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 151.1ms\n",
      "Speed: 2.0ms preprocess, 151.1ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 151.4ms\n",
      "Speed: 2.2ms preprocess, 151.4ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.0ms\n",
      "Speed: 1.9ms preprocess, 127.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 115.7ms\n",
      "Speed: 1.0ms preprocess, 115.7ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 131.3ms\n",
      "Speed: 2.5ms preprocess, 131.3ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.0ms\n",
      "Speed: 2.0ms preprocess, 127.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 127.6ms\n",
      "Speed: 3.0ms preprocess, 127.6ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.5ms\n",
      "Speed: 2.5ms preprocess, 112.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 102.8ms\n",
      "Speed: 1.5ms preprocess, 102.8ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 113.4ms\n",
      "Speed: 2.5ms preprocess, 113.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.5ms\n",
      "Speed: 2.5ms preprocess, 104.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 131.9ms\n",
      "Speed: 2.1ms preprocess, 131.9ms inference, 3.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.4ms\n",
      "Speed: 2.5ms preprocess, 112.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 111.3ms\n",
      "Speed: 3.0ms preprocess, 111.3ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 104.9ms\n",
      "Speed: 2.0ms preprocess, 104.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.1ms\n",
      "Speed: 2.0ms preprocess, 110.1ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 125.1ms\n",
      "Speed: 2.0ms preprocess, 125.1ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 132.0ms\n",
      "Speed: 2.0ms preprocess, 132.0ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 146.7ms\n",
      "Speed: 2.7ms preprocess, 146.7ms inference, 4.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 156.3ms\n",
      "Speed: 2.0ms preprocess, 156.3ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 142.8ms\n",
      "Speed: 2.0ms preprocess, 142.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 120.2ms\n",
      "Speed: 2.0ms preprocess, 120.2ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 111.0ms\n",
      "Speed: 1.0ms preprocess, 111.0ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 108.4ms\n",
      "Speed: 2.0ms preprocess, 108.4ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 121.9ms\n",
      "Speed: 2.0ms preprocess, 121.9ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.5ms\n",
      "Speed: 2.5ms preprocess, 112.5ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 106.0ms\n",
      "Speed: 2.0ms preprocess, 106.0ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 107.5ms\n",
      "Speed: 2.0ms preprocess, 107.5ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 108.0ms\n",
      "Speed: 2.0ms preprocess, 108.0ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 111.4ms\n",
      "Speed: 2.0ms preprocess, 111.4ms inference, 2.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.3ms\n",
      "Speed: 2.0ms preprocess, 112.3ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 110.8ms\n",
      "Speed: 2.0ms preprocess, 110.8ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 116.7ms\n",
      "Speed: 2.0ms preprocess, 116.7ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 130.1ms\n",
      "Speed: 2.5ms preprocess, 130.1ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 136.5ms\n",
      "Speed: 1.0ms preprocess, 136.5ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 125.1ms\n",
      "Speed: 1.2ms preprocess, 125.1ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 109.9ms\n",
      "Speed: 1.5ms preprocess, 109.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 113.4ms\n",
      "Speed: 2.0ms preprocess, 113.4ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 109.5ms\n",
      "Speed: 1.5ms preprocess, 109.5ms inference, 4.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 153.6ms\n",
      "Speed: 3.0ms preprocess, 153.6ms inference, 3.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 person, 112.4ms\n",
      "Speed: 2.0ms preprocess, 112.4ms inference, 3.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Mean Inference Time: 177.8286 ms\n"
     ]
    }
   ],
   "source": [
    "def segment_webcam():\n",
    "    # Start video capture from webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    inference_times = []\n",
    "\n",
    "    while True:\n",
    "        # Read frame from webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "        results = model(frame, task='segment')\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Calculate and store inference time\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        # Extract masks, bounding boxes, etc.\n",
    "        annotated_frame = results[0].plot()  # Annotated frame with segmentations\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow(f'{model_name} Real-Time Segmentation', annotated_frame)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the webcam and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Compute and print mean inference time\n",
    "    if inference_times:\n",
    "        mean_inference_time = sum(inference_times) / len(inference_times)\n",
    "        print(f\"Mean Inference Time: {mean_inference_time * 1000:.4f} ms\")\n",
    "        return mean_inference_time\n",
    "    \n",
    "mean_inference_time = segment_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time: 177.8286 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Inference Time: {mean_inference_time * 1000:.4f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model export and quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose here is to export the model to the onnx format and quantize it in order to reduce its size and inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export to onnx using Ultralytics export function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: onnxruntime flutter plugin requires opset version < 19 and ONNX IR version < 10, see [ONNXRuntime compatibility](https://onnxruntime.ai/docs/reference/compatibility.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.28  Python-3.11.3 torch-2.5.1+cpu CPU (Intel Core(TM) i7-8750H 2.20GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolo11n-seg.pt' with input shape (1, 3, 320, 320) BCHW and output shape(s) ((1, 116, 2100), (1, 32, 80, 80)) (5.9 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.14.1 opset 17...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.36...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.0s, saved as 'yolo11n-seg.onnx' (11.1 MB)\n",
      "\n",
      "Export complete (3.2s)\n",
      "Results saved to \u001b[1mC:\\Users\\33650\\Documents\\M2IA2\\Computer_vision\\seg_project\u001b[0m\n",
      "Predict:         yolo predict task=segment model=yolo11n-seg.onnx imgsz=320  \n",
      "Validate:        yolo val task=segment model=yolo11n-seg.onnx imgsz=320 data=/ultralytics/ultralytics/cfg/datasets/coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolo11n-seg.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.export(format=\"onnx\", opset=17, imgsz=320)  # Export ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize the model using ONNXRuntime quantizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model exported to onnx format, we can quantize it using the ONNXRuntime quantizer. (cf [ONNXRuntime quantization](https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html))  \n",
    "  \n",
    "There are 2 kind of quantization:\n",
    "- Dynamic quantization: computes quantization parameters (zero-point and scaling factors) on the fly. These calculations increase the cost of inference, while usually achieve higher accuracy comparing to static ones. This type of quantization is usually more suitable for transformers and linear layers.\n",
    "- Static quantization: quantization parameters are calculated during the quantization process, not during inference. It first runs the model using a set of images called calibration data. During these runs, we compute the quantization parameters for each activations. These quantization parameters are written as constants to the quantized model and used for all inputs. Static quantization is recommended for CNN models.\n",
    "\n",
    "Since our model is a CNN based model, we use static quantization.  \n",
    "  \n",
    "During the calibration process, we need to preprocess the images the same way it will be done during inference.  \n",
    "We use the CalibrationDataReader class from ONNXRuntime (cf [examples](https://github.com/microsoft/onnxruntime-inference-examples/tree/main/quantization)) with a set of 10 random images from the COCO dataset available in the [calibration_img](./calibration_img/) (the images should be similar to the ones the model will get as input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import CalibrationDataReader\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # Resize and pad image while maintaining aspect ratio\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # width and height padding\n",
    "    dw /= 2  # divide padding into two sides\n",
    "    dh /= 2\n",
    "\n",
    "    im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "class YOLOCalibrationDataReader(CalibrationDataReader):\n",
    "    def __init__(self, calibration_image_folder, model_input_name, input_size=(640, 640)):\n",
    "        self.data_dir = calibration_image_folder\n",
    "        self.model_input_name = model_input_name\n",
    "        self.input_size = input_size\n",
    "        self.image_files = os.listdir(self.data_dir)\n",
    "        self.preprocess_images()\n",
    "        self.data_iter = iter(self.input_batches)\n",
    "\n",
    "    def preprocess_images(self):\n",
    "        self.input_batches = []\n",
    "        for image_file in self.image_files:\n",
    "            image_path = os.path.join(self.data_dir, image_file)\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            # Preprocess the image (resize, normalize, etc.)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image, _, _ = letterbox(image, new_shape=self.input_size)\n",
    "            image = image.astype(np.float32) / 255.0\n",
    "            image = np.expand_dims(image.transpose(2, 0, 1), axis=0)\n",
    "            batch = {self.model_input_name: image}\n",
    "            self.input_batches.append(batch)\n",
    "\n",
    "    def get_next(self):\n",
    "        return next(self.data_iter, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform static quantization.  \n",
    "The last layer is very sensitive to quantization, therefore we exclude it from the quantization process to get good predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_static, CalibrationMethod, QuantType, QuantFormat, quant_pre_process\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Define paths\n",
    "model_fp32 = 'yolo11n-seg.onnx'\n",
    "model_preprocessed = 'yolo11n-seg-preprocessed.onnx'\n",
    "model_quant = 'yolo11n-seg-quantized.onnx'\n",
    "\n",
    "session = ort.InferenceSession(model_fp32, providers=['CPUExecutionProvider'])\n",
    "input_shape = session.get_inputs()[0].shape  # [batch_size, channels, height, width]\n",
    "input_height, input_width = input_shape[2], input_shape[3]\n",
    "\n",
    "# Create data reader\n",
    "calibration_data_reader = YOLOCalibrationDataReader(\n",
    "    calibration_image_folder='./calibration_img',\n",
    "    model_input_name=session.get_inputs()[0].name,\n",
    "    input_size=(input_height, input_width)\n",
    ")\n",
    "\n",
    "# Quantization configuration\n",
    "quantization_config = {\n",
    "    'activation_type': QuantType.QUInt8,  # Quantize activations to uint8\n",
    "    'weight_type': QuantType.QInt8,      # Quantize weights to int8\n",
    "    'quant_format': QuantFormat.QOperator,               # Quantization format\n",
    "    'per_channel': False,                 # Enable per-channel quantization\n",
    "    'calibrate_method': CalibrationMethod.MinMax,\n",
    "}\n",
    "\n",
    "quant_pre_process(model_fp32, model_preprocessed, skip_symbolic_shape=True)\n",
    "\n",
    "nodes_to_exclude = [\"/model.23/Concat_6\"]\n",
    "\n",
    "# Perform static quantization\n",
    "quantize_static(\n",
    "    model_input=model_preprocessed,\n",
    "    model_output=model_quant,\n",
    "    calibration_data_reader=calibration_data_reader,\n",
    "    quant_format=quantization_config['quant_format'],\n",
    "    activation_type=quantization_config['activation_type'],\n",
    "    weight_type=quantization_config['weight_type'],\n",
    "    per_channel=quantization_config['per_channel'],\n",
    "    calibrate_method=quantization_config['calibrate_method'],\n",
    "    nodes_to_exclude=nodes_to_exclude\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-time inference using float model\n",
    "\n",
    "The onnx model does not include the post-processing (Non-max-suppression) contrary to the torch ultralytics model.  \n",
    "Therefore, we need to perform post-processing manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time: 21.39 ms\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torchvision\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "\n",
    "iou_threshold = 0.7 # Lower values result in fewer detections by eliminating overlapping boxes, useful for reducing duplicates.\n",
    "conf_threshold = 0.25 # Sets the minimum confidence threshold for detections. Objects detected with confidence below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n",
    "\n",
    "COCO_CLASS_NAMES = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train',\n",
    "    'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "    'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard',\n",
    "    'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard',\n",
    "    'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork',\n",
    "    'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange',\n",
    "    'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair',\n",
    "    'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv',\n",
    "    'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave',\n",
    "    'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "    'scissors', 'teddy bear', 'hair dryer', 'toothbrush'\n",
    "]\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # Resize and pad image while maintaining aspect ratio\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # width and height padding\n",
    "    dw /= 2  # divide padding into two sides\n",
    "    dh /= 2\n",
    "\n",
    "    im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def postprocess(frame, outputs, input_width, input_height, orig_shape, ratio, dw, dh, class_names, colors):\n",
    "    # Extract outputs\n",
    "    predictions = outputs[0]  # Shape: [1, 116, 8400]\n",
    "    mask_protos = outputs[1]  # Shape: [1, 32, 160, 160]\n",
    "\n",
    "    # Transpose and reshape predictions\n",
    "    predictions = predictions[0].transpose(1, 0)  # Shape: [8400, 116]\n",
    "    mask_protos = mask_protos[0]  # Shape: [32, 160, 160]\n",
    "\n",
    "    # Parse predictions\n",
    "    num_classes = 80  # Number of classes (COCO dataset)\n",
    "    num_mask_coeffs = predictions.shape[1] - 4 - num_classes  # Should be 32\n",
    "\n",
    "    # Extract boxes, class scores, and mask coefficients\n",
    "    boxes = predictions[:, :4]\n",
    "    class_scores = predictions[:, 4 : 4 + num_classes]\n",
    "    mask_coeffs = predictions[:, 4 + num_classes :]\n",
    "\n",
    "    # Compute final scores and class IDs\n",
    "    scores = class_scores.max(axis=1)\n",
    "    class_ids = class_scores.argmax(axis=1)\n",
    "\n",
    "    # Filter out low-confidence detections\n",
    "    keep = scores > conf_threshold\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    class_ids = class_ids[keep]\n",
    "    mask_coeffs = mask_coeffs[keep]\n",
    "\n",
    "    #print(f\"Number of detections before NMS: {len(boxes)}\")\n",
    "\n",
    "    if boxes.shape[0] == 0:\n",
    "        # No detections\n",
    "        return frame\n",
    "\n",
    "    # Adjust boxes to original image scale\n",
    "    boxes[:, 0] = (boxes[:, 0] - dw) / ratio  # x_center\n",
    "    boxes[:, 1] = (boxes[:, 1] - dh) / ratio  # y_center\n",
    "    boxes[:, 2] /= ratio  # width\n",
    "    boxes[:, 3] /= ratio  # height\n",
    "\n",
    "    # Convert boxes from (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
    "    x1 = boxes[:, 0] - boxes[:, 2] / 2\n",
    "    y1 = boxes[:, 1] - boxes[:, 3] / 2\n",
    "    x2 = boxes[:, 0] + boxes[:, 2] / 2\n",
    "    y2 = boxes[:, 1] + boxes[:, 3] / 2\n",
    "    boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)\n",
    "\n",
    "    # Clip boxes to image boundaries\n",
    "    orig_height, orig_width = orig_shape\n",
    "    boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clip(0, orig_width - 1)\n",
    "    boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clip(0, orig_height - 1)\n",
    "\n",
    "    # Perform Non-Maximum Suppression\n",
    "    indices = torchvision.ops.nms(\n",
    "        torch.from_numpy(boxes_xyxy).float(), torch.from_numpy(scores).float(), iou_threshold\n",
    "    )\n",
    "    indices = indices.numpy()\n",
    "\n",
    "    boxes_xyxy = boxes_xyxy[indices]\n",
    "    scores = scores[indices]\n",
    "    class_ids = class_ids[indices]\n",
    "    mask_coeffs = mask_coeffs[indices]\n",
    "\n",
    "    # Debugging: Print the number of detections\n",
    "    #print(f\"Number of detections after NMS: {len(indices)}\")\n",
    "\n",
    "    # Compute masks\n",
    "    mask_protos_flat = mask_protos.reshape(mask_protos.shape[0], -1)  # [32, H*W]\n",
    "    masks = sigmoid(np.dot(mask_coeffs, mask_protos_flat))  # [N, H*W]\n",
    "    masks = masks.reshape(-1, mask_protos.shape[1], mask_protos.shape[2])  # [N, H, W]\n",
    "\n",
    "    # Resize masks to the size of the preprocessed image\n",
    "    masks_resized = np.array(\n",
    "        [cv2.resize(mask, (input_width, input_height), interpolation=cv2.INTER_LINEAR) for mask in masks]\n",
    "    )\n",
    "\n",
    "    # Remove padding from masks\n",
    "    dh_int, dw_int = int(round(dh)), int(round(dw))\n",
    "    masks_cropped = masks_resized[:, dh_int:input_height - dh_int, dw_int:input_width - dw_int]\n",
    "\n",
    "    # Resize masks to the original image size\n",
    "    masks_resized = np.array(\n",
    "        [cv2.resize(mask, (orig_width, orig_height), interpolation=cv2.INTER_LINEAR) for mask in masks_cropped]\n",
    "    )\n",
    "\n",
    "    # Apply threshold to get binary masks\n",
    "    masks_bin = (masks_resized > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Overlay masks on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    for box, mask, class_id in zip(boxes_xyxy.astype(int), masks_bin, class_ids):\n",
    "        color = colors[class_id].tolist()\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Apply mask\n",
    "        mask_3d = np.stack([mask] * 3, axis=-1)\n",
    "        annotated_frame = np.where(\n",
    "            mask_3d, annotated_frame * 0.5 + np.array(color) * 0.5, annotated_frame\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Put class label\n",
    "        class_name = class_names[class_id]\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            class_name,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            color,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "def segment_webcam_onnx():\n",
    "    # Load the ONNX model\n",
    "    # session = ort.InferenceSession('yolo11n-seg.onnx', providers=['CUDAExecutionProvider']) # Use CUDA for GPU acceleration (if available)\n",
    "    session = ort.InferenceSession('yolo11n-seg.onnx', providers=['CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape  # [batch_size, channels, height, width]\n",
    "    input_height, input_width = input_shape[2], input_shape[3]\n",
    "\n",
    "    #model_meta = session.get_modelmeta().custom_metadata_map[\"names\"]\n",
    "    class_names = COCO_CLASS_NAMES\n",
    "    #class_names = literal_eval(model_meta)\n",
    "\n",
    "    # Generate random colors for each class\n",
    "    COLORS = np.random.randint(0, 255, size=(len(class_names), 3), dtype='uint8')\n",
    "\n",
    "    # Start video capture from the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    inference_times = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        orig_height, orig_width = frame.shape[:2]\n",
    "\n",
    "        # Preprocess the frame\n",
    "        img, ratio, (dw, dh) = letterbox(frame, (input_width, input_height))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        img = np.expand_dims(img.transpose(2, 0, 1), axis=0)  # HWC to CHW and add batch dimension\n",
    "\n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        outputs = session.run(None, {input_name: img})\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        # Postprocess outputs\n",
    "        annotated_frame = postprocess(frame, outputs, input_width, input_height, orig_shape=(orig_height, orig_width), ratio=ratio, dw=dw, dh=dh, class_names=class_names, colors=COLORS)\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow('ONNX Real-Time Segmentation', annotated_frame)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Calculate and print mean inference time\n",
    "    mean_inference_time_float_model = np.mean(inference_times)\n",
    "    print(f\"Mean Inference Time: {mean_inference_time_float_model * 1000:.2f} ms\")\n",
    "    return mean_inference_time_float_model\n",
    "\n",
    "mean_inference_time_float_model = segment_webcam_onnx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-time inference using quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time: 19.87 ms\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "import torchvision\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "\n",
    "iou_threshold = 0.7 # Lower values result in fewer detections by eliminating overlapping boxes, useful for reducing duplicates.\n",
    "conf_threshold = 0.25 # Sets the minimum confidence threshold for detections. Objects detected with confidence below this threshold will be disregarded. Adjusting this value can help reduce false positives.\n",
    "\n",
    "\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    # Resize and pad image while maintaining aspect ratio\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # width and height padding\n",
    "    dw /= 2  # divide padding into two sides\n",
    "    dh /= 2\n",
    "\n",
    "    im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )  # add border\n",
    "    return im, ratio, (dw, dh)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def postprocess(frame, outputs, input_width, input_height, orig_shape, ratio, dw, dh, class_names, colors):\n",
    "    # Extract outputs\n",
    "    predictions = outputs[0]  # Shape: [1, 116, 8400]\n",
    "    mask_protos = outputs[1]  # Shape: [1, 32, 160, 160]\n",
    "\n",
    "    # Transpose and reshape predictions\n",
    "    predictions = predictions[0].transpose(1, 0)  # Shape: [8400, 116]\n",
    "    mask_protos = mask_protos[0]  # Shape: [32, 160, 160]\n",
    "\n",
    "    # Parse predictions\n",
    "    num_classes = 80  # Number of classes (COCO dataset)\n",
    "    num_mask_coeffs = predictions.shape[1] - 4 - num_classes  # Should be 32\n",
    "\n",
    "    # Extract boxes, class scores, and mask coefficients\n",
    "    boxes = predictions[:, :4]\n",
    "    class_scores = predictions[:, 4 : 4 + num_classes]\n",
    "    mask_coeffs = predictions[:, 4 + num_classes :]\n",
    "\n",
    "    # Compute final scores and class IDs\n",
    "    scores = class_scores.max(axis=1)\n",
    "    class_ids = class_scores.argmax(axis=1)\n",
    "\n",
    "    # Filter out low-confidence detections\n",
    "    keep = scores > conf_threshold\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    class_ids = class_ids[keep]\n",
    "    mask_coeffs = mask_coeffs[keep]\n",
    "\n",
    "    #print(f\"Number of detections before NMS: {len(boxes)}\")\n",
    "\n",
    "    if boxes.shape[0] == 0:\n",
    "        # No detections\n",
    "        return frame\n",
    "\n",
    "    # Adjust boxes to original image scale\n",
    "    boxes[:, 0] = (boxes[:, 0] - dw) / ratio  # x_center\n",
    "    boxes[:, 1] = (boxes[:, 1] - dh) / ratio  # y_center\n",
    "    boxes[:, 2] /= ratio  # width\n",
    "    boxes[:, 3] /= ratio  # height\n",
    "\n",
    "    # Convert boxes from (x_center, y_center, w, h) to (x1, y1, x2, y2)\n",
    "    x1 = boxes[:, 0] - boxes[:, 2] / 2\n",
    "    y1 = boxes[:, 1] - boxes[:, 3] / 2\n",
    "    x2 = boxes[:, 0] + boxes[:, 2] / 2\n",
    "    y2 = boxes[:, 1] + boxes[:, 3] / 2\n",
    "    boxes_xyxy = np.stack([x1, y1, x2, y2], axis=1)\n",
    "\n",
    "    # Clip boxes to image boundaries\n",
    "    orig_height, orig_width = orig_shape\n",
    "    boxes_xyxy[:, [0, 2]] = boxes_xyxy[:, [0, 2]].clip(0, orig_width - 1)\n",
    "    boxes_xyxy[:, [1, 3]] = boxes_xyxy[:, [1, 3]].clip(0, orig_height - 1)\n",
    "\n",
    "    # Perform Non-Maximum Suppression\n",
    "    indices = torchvision.ops.nms(\n",
    "        torch.from_numpy(boxes_xyxy).float(), torch.from_numpy(scores).float(), iou_threshold\n",
    "    )\n",
    "    indices = indices.numpy()\n",
    "\n",
    "    boxes_xyxy = boxes_xyxy[indices]\n",
    "    scores = scores[indices]\n",
    "    class_ids = class_ids[indices]\n",
    "    mask_coeffs = mask_coeffs[indices]\n",
    "\n",
    "    # Debugging: Print the number of detections\n",
    "    #print(f\"Number of detections after NMS: {len(indices)}\")\n",
    "\n",
    "    # Compute masks\n",
    "    mask_protos_flat = mask_protos.reshape(mask_protos.shape[0], -1)  # [32, H*W]\n",
    "    masks = sigmoid(np.dot(mask_coeffs, mask_protos_flat))  # [N, H*W]\n",
    "    masks = masks.reshape(-1, mask_protos.shape[1], mask_protos.shape[2])  # [N, H, W]\n",
    "\n",
    "    # Resize masks to the size of the preprocessed image\n",
    "    masks_resized = np.array(\n",
    "        [cv2.resize(mask, (input_width, input_height), interpolation=cv2.INTER_LINEAR) for mask in masks]\n",
    "    )\n",
    "\n",
    "    # Remove padding from masks\n",
    "    dh_int, dw_int = int(round(dh)), int(round(dw))\n",
    "    masks_cropped = masks_resized[:, dh_int:input_height - dh_int, dw_int:input_width - dw_int]\n",
    "\n",
    "    # Resize masks to the original image size\n",
    "    masks_resized = np.array(\n",
    "        [cv2.resize(mask, (orig_width, orig_height), interpolation=cv2.INTER_LINEAR) for mask in masks_cropped]\n",
    "    )\n",
    "\n",
    "    # Apply threshold to get binary masks\n",
    "    masks_bin = (masks_resized > 0.5).astype(np.uint8)\n",
    "\n",
    "    # Overlay masks on the frame\n",
    "    annotated_frame = frame.copy()\n",
    "    for box, mask, class_id in zip(boxes_xyxy.astype(int), masks_bin, class_ids):\n",
    "        color = colors[class_id].tolist()\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Apply mask\n",
    "        mask_3d = np.stack([mask] * 3, axis=-1)\n",
    "        annotated_frame = np.where(\n",
    "            mask_3d, annotated_frame * 0.5 + np.array(color) * 0.5, annotated_frame\n",
    "        ).astype(np.uint8)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "        # Put class label\n",
    "        class_name = class_names[class_id]\n",
    "        cv2.putText(\n",
    "            annotated_frame,\n",
    "            class_name,\n",
    "            (x1, y1 - 10),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.5,\n",
    "            color,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "    return annotated_frame\n",
    "\n",
    "def segment_webcam_onnx():\n",
    "    # Load the ONNX model\n",
    "    # session = ort.InferenceSession('yolo11n-seg.onnx', providers=['CUDAExecutionProvider']) # Use CUDA for GPU acceleration (if available)\n",
    "    session = ort.InferenceSession('yolo11n-seg-quantized.onnx', providers=['CPUExecutionProvider'])\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    input_shape = session.get_inputs()[0].shape  # [batch_size, channels, height, width]\n",
    "    input_height, input_width = input_shape[2], input_shape[3]\n",
    "\n",
    "    model_meta = session.get_modelmeta().custom_metadata_map[\"names\"]\n",
    "    class_names = literal_eval(model_meta)\n",
    "\n",
    "    # Generate random colors for each class\n",
    "    COLORS = np.random.randint(0, 255, size=(len(class_names), 3), dtype='uint8')\n",
    "\n",
    "    # Start video capture from the webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    inference_times = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        orig_height, orig_width = frame.shape[:2]\n",
    "\n",
    "        # Preprocess the frame\n",
    "        img, ratio, (dw, dh) = letterbox(frame, (input_width, input_height))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img.astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        img = np.expand_dims(img.transpose(2, 0, 1), axis=0)  # HWC to CHW and add batch dimension\n",
    "\n",
    "        # Run inference\n",
    "        start_time = time.time()\n",
    "        outputs = session.run(None, {input_name: img})\n",
    "        end_time = time.time()\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        # Postprocess outputs\n",
    "        annotated_frame = postprocess(frame, outputs, input_width, input_height, orig_shape=(orig_height, orig_width), ratio=ratio, dw=dw, dh=dh, class_names=class_names, colors=COLORS)\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow('ONNX Real-Time Segmentation', annotated_frame)\n",
    "\n",
    "        # Press 'q' to exit the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Calculate and print mean inference time\n",
    "    mean_inference_time_quantized_model = np.mean(inference_times)\n",
    "    print(f\"Mean Inference Time: {mean_inference_time_quantized_model * 1000:.2f} ms\")\n",
    "    return mean_inference_time_quantized_model\n",
    "\n",
    "mean_inference_time_quantized_model = segment_webcam_onnx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should observe that quantized model inference time is lower than its float counterpart (although it's not a huge difference for this model, further investigation needs to be done).  \n",
    "Also, if you check the size of the quantized model should be almost 4 times smaller than its float counterpart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Inference Time (Float Model): 21.62 ms\n",
      "Mean Inference Time (Quantized Model): 23.24 ms\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean Inference Time (Float Model): {mean_inference_time_float_model * 1000:.2f} ms\")\n",
    "print(f\"Mean Inference Time (Quantized Model): {mean_inference_time_quantized_model * 1000:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Computer_vision-hBGdW_dH",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
